%!TEX root=main.tex
\chapter{Related Work}
\label{cha:relatedwork}

\begin{itemize}
	\item Parallel dataflow systems
	\begin{itemize}
		\item MapReduce
		\item Stratosphere
		\item Spark
	\end{itemize}
	\item Specialized distributed computing frameworks
	\begin{itemize}
		\item Pegasus
		\item Datalog
		\item Pregel
		\item Hyracks
		\item Apache Hama
	\end{itemize}
	\item Matlab parallelization
\end{itemize}

Pages $\approx$ 10 - 15

The challenges entailed by harnessing vast amounts of data propelled much research on the field of distributed computing as well as databases to subdue the looming data flood.
In recent years a couple of different programming paradigms and frameworks emerged, each with the goal to tackle the aforementioned problems.
The different trends can be subsumed into the following categories: Distributed data processing systems, distributed numerical computing systems, database management systems, specialized distributed computing frameworks and explicit parallelization.

\section{Distributed data processing systems}

In the last decade, MapReduce~\cite{dean:c2008a}, created by Google, has been one of the most influential developments in the domain of distributed data processing systems.
Their novel approach to look at the problem of parallelizing programs was so intriguing and successful that a whole bunch of researchers and engineers jumped on the bandwagon and spurred a whole new area of research.
Initially, MapReduce was created because of the need to process a constantly increasing amount of data.
Before, the common way to do processing tasks, such as index generation, data-mining and web log parsing, was to write specialized programs.
Often, these programs needed to be parallelized, fault-tolerant, support load-balancing and data-distribution to work properly.
Not only does this require a high level of expertise but it also inflicts significant costs in terms of labour and time.
Therefore \citeauthor{dean:c2008a} conceived MapReduce.

MapReduce is inspired by the higher-order functions \emph{map} and \emph{reduce} which can be found in many functional programming languages.
In the MapReduce framework, the user only has to provide code for the map and the reduce function.
The description for the map and reduce function as well as the input data constitute a MapReduce job.
The semantics is the following:
Given a set of key-value pairs, map is called for each pair indenpendently and produces a set of intermediate key-value pairs.
How this set is generated is defined by a user defined function (\emph{UDF}).
After the map call, all key-value pairs are grouped by their key value.
Reduce is then called for each of these groups and generates a set of result key-value pairs.
Even though this abstraction appears to be quite simple it is surprisingly powerful and a lot of algorithms can be expressed this way.

The execution model works as follows:
The input data is stored in the Google file system~\cite{ghemawat:2003a} (\emph{GFS}), a distributed file system.
Internally the data is divided input splits which are stored replicatedly across the worker nodes.
Each input split is processed by a map task.
The map tasks are assigned to worker nodes which have free mapper slots.
Since the worker nodes also contain the input data, the MapReduce scheduler tries to assign the tasks to worker nodes storing the input split.
If this is not possible, the scheduler tries to select a node which is as close as possible to the data with respect to the network topology.
Thereby, the required network communication will be minimized.

Once the map task is finished it has produced a set of intermediate results which is partitioned according to a given hash function.
Each partition is stored locally in a temporary file and serves as the input of a reduce task.
A reduce task which is spawned on a worker node retrieves its partitions from all map tasks.
This shuffle step inflicts considerable network communication overhead.
Then the key-value pairs are sorted with respect to the key value.
And finally the reduce function will be called on each group of key-value pairs.
The result of the reduce tasks will be written to the distributed file system from where it can be used for another MapReduce job.

Each map call is executed independently and thus the map phase is embarrassingly parallel.
Once the shuffle phase is done, the reduce functions can also be executed in parallel.
Therefore, it is possible to automatically parallelize the execution of a MapReduce job and thus freeing the user from this tedious and cumbersome task.
By having more map and reduce tasks than worker nodes, the framework achieves good load balancing, because the system can spread them evenly across all nodes.
Furthermore, one can even achieve a high flexibility by defining many but short lasting tasks.
The data distribution is controlled by GFS and thus it is no longer a concern of the user.
Yet, it can be influenced indirectly by specifying the replication factor, for example.
MapReduce also implements fault-tolerance by re-executing the failed tasks and those tasks whose input data is needed again but not retrievable.
The system detects task failures by maintaining for each task an internal state.
Moreover, it recurrently pings the worker nodes to check their health.
If a worker nodes does not respond in a certain amount of time, all tasks scheduled to this worker node will be set to failed and eventually rescheduled.

MapReduce is a renunciation from the common programming paradigm by confining the user to the map and reduce function.
The restricted model allows \citeauthor{dean:c2008a} to efficiently encapsulate automatic parallelization, load balancing, data distribution and fault-tolerance into the framework.
Therefore, the user no longer has to struggle with these technicalities and can instead concentrate on writing productive code.
The ease of use and the fact that MapReduce scales well to clusters of thousands of machines are the reason why the concept is so appealing.

However, there are also some deficiencies.
MapReduce only offers map and reduce as higher-order functions.
Several operations, for example, a join between two data sets, are complicated to express with only these two functions.
Furthermore, writing all results between two distinct phases to disk causes a lot of I/O, which is slow compared to keeping the results in memory.
This becomes explicitely apparent if one operates on a data set in an iterative manner.

Due to these problems a new class of systems have recently been developed which can be considered as generalizations of MapReduce.
One of them is Stratosphere~\cite{battre:2010a}, a distributed computing framework which employs PACTs~\cite{alexandrov:2011a} (parallel contracts).
Stratosphere adds additional $2^{nd}$ level functions, which are called input contracts, in order to improve the expressiveness and efficiency of the MapReduce paradigm.
Furthermore, it adds the concept of output contracts which annotate input contracts with certain properties, such as key uniqueness, record cardinality or constant fields.
These properties are exploited by a cost-based optimizer to select the most efficient execution plan.
Recently, the framework has been extended to support bulk and incremental iterations~\cite{ewen:pve2012a}, which makes it applicable for machine learning and complex data analysis tasks.

The new higher-level functions are \emph{join}, \emph{cross} and \emph{coGroup}.
The join operator joins two multisets of inputs $A$ and $B$ with respect to a key value.
This means that a UDF is called for each pair $(a,b)$ with $a\in A$ and $b\in B$ with $key(a)=key(b)$.
The cross operator can be understood as the cartesian product.
Given two input multisets $A$ and $B$, cross calls the UDF for each pair $(a,b)$ with $a\in A$ and $b\in B$.
The coGroup operator acts a little bit differently.
It also takes 2 input multisets $A$ and $B$.
The operator groups the elements of $A$ and $B$ according to their keys and joins the grouped submultisets.
In other words, the UDF is called for each pair of multisets $(A^\prime, B^\prime)$ with $A^\prime \subseteq A \wedge B^\prime \subseteq B \wedge |keyset(A^\prime \cup B^\prime)| = 1$ and $keyset(A^\prime \cup B^\prime) \cap keyset((A \cup B) \setminus (A^\prime \cup B^\prime)) = \emptyset$.
The function $keyset$ is the set of appearing keys in a set: $keyset(X) \coloneqq \{ key(x) \mid x \in X \}$.

These operators allow to express certain problems occurring in data analysis more elegantly and succintly.
Additionally it gives the system a higher level of abstraction on which it can apply optimizations.
Consider, for example, the join operator.
In MapReduce the straightforward implementation is the following:
First one has to unify both input sources in the map phase, adding a tag in order to distinguish them in the reduce phase.
In the reduce UDF, the input sources are separated according to these auxiliary variables and then joined manually.
For the join operation all entries with the same key have to loaded into memory because, a priori, the entries are not sorted according to their keys.
This can cause some serious thrashing.
Since the join logic is hidden within the UDFs, there is no way for the system to automatically optimize for this operation.
Yet, there are possibilities to optimize join~\cite{blanas:2010a} and multiway join~\cite{afrati:2010a} operations on MapReduce.
But they either require extensive hand-tuning by the user or extensions to the MapReduce programming model to work.
In contrast, the Stratosphere system is aware of the join operation and can choose the best execution plan for it.
Depending on the input sizes of the inputs, either the sort-merge join or the hash join algorithm performs better.

Stratosphere is programed by using the native Java or Scala API to specify a dataflow.
It is the representation of a computation at the operator-level of the system.
A dataflow is an directed acyclic graph (DAG) whose nodes are the operators and edges are the input dependencies between the operators.
A transformed version of the DAG is then given to Nephele~\cite{warneke:2009a}, the execution engine, which is responsible for parallel execution.

An outstanding property of Stratosphere is that it uses database-inspired pipelining to reduce data materialization and thus costly I/O.
First of all, it chains as many operators as possible.
Chaining means that multiple operators are packed into one task so that intermediate results are directly fed to a subsequent operator on the same worker node.
For example, multiple map operations can be smoothly chained.
Furthermore, the system always tries to have deployed succeeding tasks so that the intermediate results can directly be forwarded.
In the event of memory shortage, intermediate results will be gently spilled to disk.
Additionally, Stratosphere support out-of-core algorithms such as external sort or hybridhash join to deal with massive amounts of data.

Stratosphere offers a powerful programming abstraction to easily implement parallel programs.
The extension of the MapReduce paradigm and the integration of database features make it a promising candidate to supersede MapReduce as the prevalent distributed data processing system in the future.
Also the iteration support helps to make Stratosphere a more general purpose processing system than MapReduce currently is.
However, there are still some aspects which have to be improved.
First of all, Stratosphere lacks at the time of writing this thesis a proper fault tolerance.
Secondly, it is not well geared towards supporting multi jobs and multi user scenarios.
And last but not least, the system lacks the primitives, such as linear algebra data types and their operations, to easily implement data mining and machine learning algorithms based on linear algebra.

Another distributed data processing system is Spark~\cite{zaharia:2010a}.
Spark was developed to tackle the blind spot of MapReduce, its inefficiency when it comes to iterative computations.
The system is build around resilient distributed datasets~\cite{zaharia:2012a} (RDD), a distributed memory abstraction.
It allows programmers to execute in-memory computations on a large cluster with fault-tolerance.


This allows to efficiently realize iterations within in the framework.
Spark lacks an intuitive Matlab-like language frontend and thus it requires a considerable expertise to code well performing algorithms.

There are also other notable projects trying to amend the deficiencies of MapReduce.
The HaLoop~\cite{bu:pve2010a} project adds loop support to Hadoop.
In order to speed up the iterations, it allows to cache loop invariant data.
Furthermore, it makes the task scheduler iteration-aware so that tasks of subsequent iterations are scheduled on the same worker node as their predecessors
This increases data localilty and consequently decreases the communication overhead.
This all comes with the same fault-tolerance support like the original Hadoop implementation.

A similar project is Twister~\cite{ekanayake:2010a}, which also adds loop support to the MapReduce framework.
In contrast to HaLoop, Twister support in-memory storage of intermediate data of iterations.
Furthermore, it distinguishes between static and dynamic data.
The static data does not have to be loaded from their producers for each iteration again, thus decreasing the communication overhead.
Another extension is the additional reduction phase ``combine''.
Combine can be called after a reducer to reduce the produced results into a single value, which is accessible by the user program.
This collective output can be used to steer the termination of a loop, for example.
In contrast to the distributed filesystem based communication scheme in Hadoop and MapReduce, Twister relies on a publish-subscribe system for data and communication transfer.
This further decreases the I/O overhead of storing data to disk and allows to directly send produced results from a mapper to a reducer.
However, Twister does not yet support a full functional fault-tolerance.

\section{Distributed numerical computing systems}

SystemML~\cite{ghoting:2011a} has the aim to make machine learning algorithms run on massive datasets without burdening the user with low-level implementation details and tedious hand-tuning.
In order to achieve this, it provides a declarative higher-level language, called Declarative Machine learning Language (\emph{DML}), for linear algebra operations which is automatically executed in a distributed fashion.
DML is inspired by the R~\cite{r:1993a} language which is the quasi standard among data scientists and statisticians.
The language supports matrices and scalars as basic types which is the basis of a wide variety of supervised and unsupervised machine learning algorithms.

The linear algebra operations are translated into a directed acyclic graph of high-level operators (\emph{HOP-DAG}).
This abstract representation allows to apply several optimizations such as algebraic rewrites, choice of internal matrix representation and cost-based selection of the best execution plan.

Matrices are separated into quadratic blocks whereas each block is uniquely identified by its row and column index.
The block representation has the advantage to reduce the overhead inflicted by a cell-wise representation where for each entry a pair of indices has to be stored additionally.
Assuming an entry is stored as a double requiring 8 bytes and 2 integers for the indices requiring 4 bytes each, this scheme would double the memory footprint of a matrix.
This would cause serious memory shortages and is thus not feasible.
Furthermore, the block representation allows to choose an memory-efficient internal representation depending on its sparsity.
If a block has only few non-zero entries, then it is stored in sparse matrix, otherwise a dense matrix is employed.
Depending on the choice of the internal matrix representation, SystemML picks the right block-level operations for best performance.

A crucial operation which appears in many machine learning algorithms and often inflicts the highest runtime costs is the matrix-matrix multiplication.
Therefore, SystemML offers two different execution strategies.
The replication based strategy broadcasts the smaller matrix to all worker nodes having a block of the bigger matrix.
Then the result block can be computed with these information.
The other strategy is based on the outer-product representation of a matrix-matrix multiplication.
Here, the columns of the left matrix are joined with the rows of the right matrix.
The outer-product computes a set of intermediate matrices which have to be summed up to deliver the final result.
Both strategies exhibit different runtime characteristics with respect to the file system and network costs.
File system costs are caused by reading and writing to the distributed filesystem which occurs at the beginning and ending of each map- and reduce-phase of a MapReduce job.
Network costs are the dominant factor and thus the system chooses the best matrix-matrix multiplication strategy with respect to the estimated network costs.

Once these optimizations are applied, the HOP-DAG is translated into a directed acyclic graph of low-level operators (\emph{LOP-DAG}).
These low-level operators can directly be mapped onto MapReduce jobs which represent the final execution format.
Between two MapReduce jobs the output of the former, which is often the input of the latter, is written to disk and then read again.
This causes a considerable performance loss and therefore SystemML tries to minimize the number of jobs.
It uses piggybacking to aggregate the low-level operators into as few as possible map- and reduce-phases.
Additionally, the system employs \emph{local aggregators} which reduce data in the reducer by combining produced results.
The idea is similar to the \emph{combiner}~\cite{dean:c2008a} concept and also reduces the amount of disk I/O.

SystemML proves to scale well with an increasing number of worker nodes and an increasing amount of data.
However, it lacks a proper iteration mechanism which is hardly described in the paper.
It is not specified whether only loops with a static termination criterion, such as a maximum number of iterations, or also dynamic termination criterions are supported.
In either case, the iterations have to realized within distinct MapReduce jobs.
This makes the loop very inefficient, because for each iteration the loop data has to be written to and read from disk.
Furthermore, SystemML relies on the MapReduce framework with all of its deficiencies compared to the more powerful parallel dataflow system Stratosphere or Spark.
Stratosphere and Spark support in memory storage of intermediate results which significantly speeds up loop processing for example.
But still, SystemML is a very promising project heading in the right direction to make web-scale data analytics accessible for people not familiar with distributed computing.

Ricardo~\cite{das:2010a} is part of the eXtreme Analytics Platform~\cite{balmin:jrd2013a} (\emph{XAP}) developed at IBM.
XAP is developed with the intention to support deep analytics on large-scale data and comprises several modules.
Ricardo uses existing technologies to implement a scalable system for statistical analysis.
The statistical computations are done within the R ecosystem, because it has a rich library of analytical methods and a close-knit community of statisticians, which is not eager to adopt a completely new system.
The problem, however, is that R works only on data which is stored in the computer's main memory.
Hadoop~\cite{hadoop:2008a}, an open-source implementation of MapReduce, is a data processing system which does not suffer from this problem.
Yet it lacks the statistical functionality of R.
Thus, in order to create an anlytical system which scales to tera- or even petabytes of data, the authors combine the two systems so that one system benefits from the strengths of the other system.
The idea is that the data shipment is done by Hadoop and the actual analytic computation by R.
For the integration of both systems, Jaql (JavaScript Object Notation query language), which is part of XAP, is used.

Jaql~\cite{beyer:2011a} constitutes a declarative high-level language for data-processing on Hadoop.
Jaql provides a rich set of high-level operators such as \emph{join}, \emph{group by} and \emph{transform} with which it is possible to easily and quickly define data-flows.
Furthermore, it still gives you access to the underlying MapReduce code when needed.
Therefore, it is flexible and powerful enough to be used as an interface between R and Hadoop.

The system is controlled from within a R driver process.
By using Jaql as a R-Hadoop bridge, the user can initiate the distribution, transformation and aggregation of data within Hadoop.
Furthermore, the system supports to run R code on the worker nodes as data transformations.
The calculated Hadoop results can also be collected at the R driver process once they have an appropriate size.
This makes it possible to apply statistical computations to large-scale data.
Since all is done from within R, the user does noth have to re-adapt and can profit from a huge code base of sophisticated statistical algorithms.

Ricardo implements a scalable system for deep analytics by combining the strengths of two well established systems, namely R and Hadoop.
The combination approach enables the authors to quickly come up with a working system without reinventing the wheel.
However, Hadoop is not integrated transparently into R.
The user still has to specify explicitely which data to distribute when within the cluster and how the processing data-flow looks like.
This requires a substantial understanding of the working principles of MapReduce and the applied method.
Only then the user can efficiently distinguish between parallelizable and serial parts of the algorithm.
Therefore, the system is not well suited for people only familiar with R.

Another project which tries to extend R to scale to petabytes of data is RHIPE~\cite{guha:s2012a}.
RHIPE follows the new statistical approach of divide and recombine (\emph{R\&D}).
The idea is to split the examined data up into several chunks so that they fit in the memory of a single computer (\emph{S-Step}).
Then a collection of analytical methods is applied to each chunk without communication with any other computer (\emph{W-Step}).
This makes the computation embarrassingly parallel, the simplest parallel processing.
After each chunk is evaluated, the results will be recombined in an aggregation step to create the overall analytical result (\emph{B-Step}).

This methodology strongly resembles the MapReduce principle and thus it is not surprising that the authors use the Hadoop system to execute the RHIPE program.
In fact, the W- and B-Step can be directly mapped to map and reduce tasks respectively.
The actual statistical analysis is done by R.
The user has to define for each step R code.
First, the R code is used to split the data into chunks for the S-Step.
This data is then distributed to the Hadoop worker nodes using the HDFS.
On the worker nodes the R code for the W-Step is executed in the map tasks.
This places the whole analytical power of the R system with all its proven libraries at the analyst's disposal.
And finally, the recombination in the B-Step is carried out by applying the analyst provided R code on the intermediate results of the W-Step.

The approach of RHIPE has the charm to be perfectly executable parallely due to its R\&D paradigm.
However, this strict execution pipeline constrains the analysis process considerably.
First of all, the statistical method the analyst wants to apply has to be suitable to be split up into a map phase where subsets of the data are processed independently and a reduce phase where the final result is processed from the intermediate results.
For example, it works for the mean calculation of a data set if the chunks are of equal size.
Yet, as soon as you need access to the whole data set to compute the exact result, as it is, e.g., the case for linear regression, one can in general only compute an approximation.
This might still be enough for most cases since statistics itself is by its definition an approximation.
But first, the user has to conceive the data division, the data processing and the final recombination step.
This strongly resembles programming a MapReduce job.
Since not many data analysts are familiar with the concepts of MapReduce, it might hinder them to utilize the system.

A system which integrates more seamlessly into the R ecosystem is pR~\cite{samatova:2009a} (parallel R).
Again, the goal of pR is to let the statistician compute large-scale data without requiring him to learn a new system.
pR achieves this by providing a set of specialized libraries which offer parallelized versions of different algorithms.
In this way, the user only has to switch the used library and can directly benefit from the parallel processing power.
Thus the adoption requires only little code changes which makes it cost- and labour-efficient.

For the parallelization pR utilizes several methods.
One approach is to integrate \nth{3} party libraries containing specialized code into the R session.
Often high performance computing (HPC) code is used, which is parallelized by using a message-passing system such as MPI.
This has the advantage to have well tested and efficient code.
But MPI requires to have an HPC cluster with a high throughput network to run efficiently~\cite{sur:2006a}.
Furthermore, it assumes to have exclusive access to the computing resources, which is usually not guaranteed in a multi-user environment.
Therefore, this approach is unsuitable to be used on a shared commodity cluster.

pR also offers parallelization for the \emph{lapply} method of R.
R's lapply method takes a list of values and a user-defined function and applies this function on the list of values.
Since this operation is embarrassingly parallel, it is a natural candidate for automatic parallelization.
The parallelization is again achieved by using MPI to orchestrate the distributed execution.

pR impresses with its seamless integration into R so that the parallel execution of R code comes almost for free.
The user neither has to learn a new system nor does he have to change a lot of code.
But the downside is that not all operations in R are supported to be run distributedly.
Moreover, MPI is used for the parallelization.
MPI is well suited for HPC where a single large job is execlusively run on a set of machines.
But nowadays, many data analysts share a cluster of commodity hardware and run several jobs concurrently, some of which are interactive and others take a long time to finish.
This interferes with the design of MPI.
Furthermore, MPI inherently lacks often requested properties such as fault-tolerance, elasticity and reliability.

\section{Database systems}

As the statistics community tries to enrich systems like R with massive data processing capabilities, the database community tries to extend database management systems (\emph{DBMS}) with analytical functionality.
It is already the case that the current SQL standard supports simple statistical computations such as linear regression, correlation coefficient and T-test functions.
However, the system quickly reaches its limits, as soon as you try to solve slightly more complex problems, for example, classification.
Therefore, \textcite{cohen:pve2009a} devised a set of properties a modern DBMS has to satisfy in order to meet the requirements of Big Data analysts.
In order to further integrate statistical computations into the DBMS, they showed how to implement a matrix type and the corresponding matrix operations with SQL.
Additionally, they implemented examplarily the conjugate gradient method and Mann-Whitney U test.
Due to the matrix operations the implementation is more elegant than with pure SQL.
But still, the definition of statistical functions within this system is laborious and complex compared to R.
Furthermore, there is no easy way to migrate existing R code.
This implies a huge amount of work to reimplement the algorithms statisticians are used to work with.
For these reasons, it is doubtful whether the statistics community will adapt this approach to realize large-scale data analytics.

\Textcite{stonebraker:2009a} have embarked on a similar path as \citeauthor{cohen:pve2009a} in the sense that they first attempted to specify the requirements for a science database, called \emph{SciDB}.
They identified support of multi-dimensional arrays to be essential for many scientific and industrial use cases.
Therefore, they constructed SciDB around this concept as its integral data type.
This makes it more natural to implement linear algebra operations within this system.
SciDB tries to push computations to the data to improve its scalability.
\citeauthor{stonebraker:2009a} stress similar to \citeauthor{cohen:pve2009a} the importance to operate on ``in situ'' data.
This feature is very helpful for the analyst to directly start working on the data.
Usually he has to waste time with tedious data preprocessing in order to load the data in the database.

The ideas proposed by \citeauthor{stonebraker:2009a} for a scientific database seem to be very promising.
It will be interesting to see how the multi-dimensional array data model performs in real world scenarios.
However, in the meantime it is important to further extend the statistical functionality.
Only then, the system can get adopted by the statistics community.

The RIOT~\cite{zhang:apa2009a} (R with I/O Transparency) project is aimed to incorporate the advantages of a DBMS into R.
The authors \citeauthor{zhang:apa2009a} have discovered that inefficient I/O of R causes a substantial performance loss when applied to large data sets.
Since R loads all its data into main memory, the virtual memory mechanism has to swap data to and from local disk as soon as the data amount exceeds the memory limit.
This can cause thrashing of the system which ultimately leads to poor performance.
The first approach to cope with this problem is to hand-tune the critical code sections which, however, burdens the programmer.
A second solution is to use I/O optimized libraries which tackle the problem at the intra-operational level.
This method, though, leaves out valuable chances for inter-operation optimizations.

DBMS offer I/O-efficiency and have with their query optimizer a powerful tool at hand to optimize code on an inter-operation level.
Therefore, the authors chose to solve the aforementioned problems by utilizing a DBMS, which they call RIOT-DB.
To guarantee a quick and wide adoption among R users, they paid explicit attention to a seamless integration into the R environment.
They implemented the RIOT-DB as an R package, which can be dynamically plugged in.
Furthermore it does not require any user code changes to benefit from its features.
RIOT provides the basic set of R types: Matrices, vectors and arrays.
They are implemented within the database as key-value pairs.
Furthermore, the primitive matrix and vector operations are provided so that the R user never has to face any SQL code.

The authors identified the following I/O-inefficiencies in R which can be mitigated by RIOT:
R programs consist of a set of statements which each produce a result.
Usually these results are the input for latter statements and thus need to be stored as intermediate results.
As long as the memory can hold all these values, then there is no problem.
However, if not, then the data has to materialized on disk, which is expensive.
By compiling the whole program into a database query, it can be efficiently pipelined.
The inputs for each operator are computed when needed and thus the unnecessary materialization of intermediate results is avoided.

Another inefficiency are unneeded computations which always appear if one continues the computation with a subset of the former result.
By deferring the computations until it is clear which elements have to be computed and a subsequent selective evaluation, the system does not waste any resources.
This feature is almost naturally achieved by using database views during the query construction and thus postponing the evaluation.

Data layout and consequently data access patterns constitute a further inefficiency.
In R the access patterns are inherently sequential and data layouts are static.
Yet it may be faster for certain operations, such as a matrix-matrix multipliation, to have, for example, a column- instead of a row-wise partition for the right matrix.
This would significantly reduce the number of page faults.
Unfortunately, the RIOT-DB implementation is not yet capable to apply these optimizations.

An extra improvement is the reordering of computations.
Considering a threefold matrix multiplication, it is important to choose the right execution order.
Depending on the size of the intermediate matrix, one or the other order might be infeasible to execute.
An efficient system has to able to make these kind of optimization decisions, which are similar to database query optimizations.
But again, the current RIOT-DB is not capable of doing it.

Even though matrix representations in a relational database as a set of indices and a value are inherently inefficient as shown in \cite{stonebraker:2007a}, the pipelined execution and the inter-operation level optimization of the DBMS makes RIOT in many cases faster than R.
Considerung current developments of the statistical capabilities of DBMS as well as support for matrices and vectors, it is not far-fetched to believe that RIOT will significantly improve its performance in the future.
But the system suffers currently from a non-negligible disadvantage when it comes to state of the art machine learning algorithms: Namely iteration support.
There is no way described how to efficiently realize loops.

\section{Specialized distributed computing frameworks}

Apart from these more general approaches to distributed data processing and numerical computing systems, there emerged also quite a lot of specialized systems.
By restricting oneself to a more confined domain, it is often possible to find more efficient ways to solve a problem at the expense of generality.

Pegasus~\cite{kang:2009a} is a programming model mainly intended for graph mining purposes.
It is centered around the abstraction of a generalized iterative matrix vector multiplication (GIM-V), which can be found in many graph algorithms.
The GIM-V operation can be efficiently represented by a map and a reduce task.
Consequently, Pegasus implements it on top of Hadoop.
Gilbert is supposed to contain the GIM-V operation and will not only consider the operation itself for optimization purposes but also its context of previous and succeeding operations.
Therefore, Gilbert should be a generalization of Pegasus.

\Textcite{bu:apa2012a} remarked that there exist a multitude of more or less specialized programming models for the task of distributed machine learning.
All of these systems exhibit a tight coupling of a solution's logical representation and physical representation.
This renders optimization difficult, because one is bound to the underlying runtime implementation and disregards alternative execution strategies.
Moreover, the systems are mostly disjoint which implies that each framework has to be updated in order to profit from new optimization strategies.
As a solution the authors propose to employ Datalog as a declarative language for the specification of higher level programming models such as iterative MapReduce or Pregel~\cite{malewicz:2010a} within their system.
Standard query optimization techniques are applied on this common intermediate representation and then it is transformed into a physical execution plan.
This plan is executed by Hyracks~\cite{borkar:2011a}, a data-parallel platform for data-intensive tasks.
This approach has the advantage that a wider class of machine learning algorithms are efficiently supported within the same system, thus profiting from the same underlying infrastructure.
However, at the moment the physical plans are still created by hand which makes the framework not applicable yet.

Apache Mahout~\cite{mahout:2011a} is a project offering a library of scalable machine learning algorithms.
Many algorithms use the MapReduce paradigm to achieve scalability and are written in Hadoop.
However, Mahout offers no easy way to write new algorithms by using a declarative language for example.
By using Hadoop, the actual execution plan of an algorithm has to be hand-tuned for the specific cluster and input size.

\section{Explicit parallelization}
