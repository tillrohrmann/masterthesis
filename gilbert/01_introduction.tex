%!TEX root=main.tex
\chapter{Introduction}
\label{cha:introduction}

\chapterquote{The important thing is not to stop questioning.}{Albert Einstein, (1879 - 1955)}

The Sloan Digital Sky Survey (SDSS), an imaging and spectroscopic redshift survey, was initiated in the year 2000.
Only a couple of weeks after its start, its telescope had already produced more data than in the entire history of astronomy before.
Nowadays, its archive is filled with about $140$ terabytes of sky images in different spectra.
The successor telescope of SDSS, the Large Synoptic Survey Telescope in Chile will collect this amount of data every five days, once put into operation~\cite{economist}.

The fact that we are gathering more and more data at an ever-increasing pace is not only observable in the astronomy but also in every other aspect of our daily lives.
It is estimated that we are producing $\SI{2.3}{\exa\byte}$ of information each day, which is roughly $10000$ times the data stored in the Library of Congress~\cite{agrawal:2011a}.
Actually, we have created $90\%$ of mankind's data, during the last two years~\cite{jewell:2014a}.
Rightly so, we can claim to live in the information age, where data became a ubiquitous yet precious resource for companies and people alike.

Since the beginning of the information age, our environment became more and more infused with digital technology gathering, processing, storing and broadcasting information of various kinds.
Nowadays, computing devices are almost omnipresent and many can hardly imagine a world without them anymore.
We can find information technology in our cars making our ride safer, in our washing machines controlling the proper cleansing and in our mobile phones acting as our personal assistant just to name a few.
Apart from the personal use, digital devices found their way into almost all important branches of science and industry, such as finance, engineering, health and commerce.

The information technology's key to success and the reason for its quick adoption is its unprecedented amplification in computing, storage and telecommunication capacity over the last couple of decades.
The growth has been mainly spurred by the technological progress on the fields of integrated circuits, broadcasting technology and algorithms.
Since digital devices are strongly linked to the semiconductor technology, the aforementioned capacities exhibit a similar exponential growth rate~\cite{hilbert:s2011a} as Moore~\cite{moore:1965a} predicted for the number of components on integrated circuits.
It is predicted that mankind will have produced roughly $\SI{40}{\zetta\byte}$ of data by the year of $\num{2020}$~\cite{ibmBigData:2014a}.
Recently, the term Big Data was coined to describe the steadily growing accumulation of information.

In fact, there is an ongoing debate on what Big Data actually denotes.
The sudden appearance of the term, which is more like a buzz word nowadays, might suggest that Big Data is something revolutionary.
But since its beginnings, computer science actually occupied itself with the automatic processing of data.
Even before the term Big Data came up, there were people, like the database community, who handled massive amounts of data.
Thus, one could argue that Big Data is a mere marketing word, describing an evolutionary aspect of computer science.
What people could agree on, is that Big Data solutions differ from other IT solutions in terms of \emph{Volume}, \emph{Velocity}, \emph{Variety} and \emph{Veracity}, also denoted as the four Vs.

Volume describes the aspect of storing and processing enormous amounts of data.
Of course, the concrete amount of data always has to be seen in the temporal context.
What today is considered to be Big Data is probably ridiculously small in $10$ years.
Velocity means that data is arriving more and more rapidly and that the system has to cope with that.
It might even be necessary to support streaming and achieve real-time computing, as it is required for the financial sector.
Variety alludes to the problem of integrating data of various kinds and from various sources.
It is very common to work on structured as well as unstructured data.
For example, a medical record not only consists of the diagnosis written by the doctor, but it might also include x-ray files from the last screening.
Veracity is the last dimension and says that not all data can be taken as gospel.
The system has to handle possibly incorrect and imprecise data and be able to distinguish it from reliable data.
The first three Vs were first introduced by Gartner~\cite{gartner}.

In order to develop Big Data solutions, one has to solve multifaceted problems.
The necessary technology stack comprises the recording of data, the data cleaning and meta-data generation, the representation and integration with other data sources, the analysis and modeling of the actual problem and the interpretation.
Each task for itself is highly complex and deserves an individual paper.
For us, the data analysis part is the key aspect in solving Big Data problems.
Henceforth, we will concentrate on how to gain valuable insights from a huge data set.

Once big data sets were amassed, people quickly recognized that these sets contain valuable information they only have to harness.
Unfortunately, the search for useful patterns and peculiarities strongly resembled the search for a needle in a haystack.
The quest requires sophisticated tools being able to analyze the vast amounts of data.
Most of these tools are based on statistics to extract interesting features.
It is beneficial to apply these statistical means to the complete data set instead of smaller chunks.
Even if the chunks cover the complete data set, important correlations between data points might get lost in smaller parts, treated individually.
Moreover, the statistical tools improve their descriptive and predictive results by getting fed more data.
The more data is available, the more likely it is that noise cancels out and that significant patterns manifest.
However, this comes at the price of an increased computing time which requires fast computer systems to make computations possible.

The insights gained from collected data already help to govern business decisions, improve life quality or simply create new industries from scratch.
For example, retail stores analyze their sales, customer, pricing and weather data in order to decide which products to offer or when to do a discount sale.
This not only increases the revenue of the shops but also boosts the satisfaction of the customers by getting better offers. 
Police departments try to detect probable crime sites by extracting patterns from previously recorded criminal acts and then reinforce the policemen in this region~\cite{lohr:yt2012a}.
The intelligent deployment of policemen improves security for citizens without having to hire more officers.
Hospitals analyze their patients' records and scientific studies in order to find the cancer treatment best complying with the specifics of the patient.
The individual therapy maximizes the chance of cure~\cite{watson:2013a}.
Another example, showing the benefits of data analysis, made it even into a Hollywood film.
The film Moneyball is based on the true story of the Oakland Athletics baseball team which build an elite team in an unfavorable financial situation.
They employed sophisticated data analysis methods to spot underestimated players which they could cheaply recruit.
That marked the start of sports statistics, which are nowadays a common tool for professional teams.
These examples emphasize the importance and utility of information gained with analytic tools from gathered data.
Interestingly, the IDC, an international market research firm, estimates that only $0.5\%$ of the globally collected data is harnessed~\cite{gantz:iaf2012a}.
They further state that about $23\%$ of today's data is worth being analyzed.
Thus, there is still some potential left for improvements.

What are the reasons for the huge gap between actual and dormant exploitation?
One of the reasons is that we are lacking the tools to keep up the pace of how fast data is created and collected.
Our analyzing methods do not scale well for the vast data sets.
The more data a system has to process the longer it will take to finish.
Since the data is growing at an exponential rate, our analytic capacities have to improve at a similar speed to keep computations feasible.

In order to decrease the runtime of our analytic tools there are two adjusting screws.
First of all, there are the algorithms.
By finding an algorithm with a lower runtime complexity than an existing algorithm, e.g., for clustering, we can drastically decrease the required time.
However, it is strongly doubted that it is possible to develop better algorithms for certain problems.
In some cases, it is even proven that there exists a lower bound for any algorithm solving the problem.
For example, it can be shown that the complexity of a sorting algorithm, which is based on comparing two of its elements to obtain their ordering, is in $\Omega\left(n\log n\right)$ with $n$ being the number of elements.
Thus, certain problems have an inherent limitation of how fast they can be computed.

The other way to speed the analytic tools up is to make the computer running them faster by vertical or horizontal scaling.
To scale vertically means that we add more resources to a single computer.
For example, the main memory or the frequency of a computer could be increased, giving them more computational power.
In contrast to that, horizontal scaling means to add more computer nodes to a system.
By having more than one node, the work can be split up and distributed across the nodes.
Since each split is smaller than the original problem, the computer can finish it faster.

In recent years, we have seen that clock rates of CPUs stagnated.
Before, there was a simple receipt to increase the computing power of micro-controllers; increase the clock rate, which demands more power, and shrink the channel widths to mitigate for the increased power consumption.
However, the shrinkage induce the problem of leakage, which increases the power demand again.
The consumed power is limited by the amount of energy you can dissipate and thus there is a technological limit for the increase of clock rate.
When it became clear that the micro-controller would hit this so-called power wall, one duplicated the micro-controller's functionality to support simultaneous execution of multiple applications and to harness the inherent parallelism of programs.

The emerging multi-core and distributed systems pose new challenges for programmers, since now they have to know about locking, deadlocks, race-conditions and inter-process communication in order to make most of the available hardware.
Furthermore, they have to be able to reason about interwoven parallel control flows.
Due to this, parallel program development is highly cumbersome and error-prone.
Therefore, new programming models are conceived which relieved the programmer from the tedious low-level tasks related to parallelization such as load-balancing, scheduling of parallel tasks and fault recovery.
Instead, the programmer can concentrate on the actual algorithm and the goal he wants to achieve.

These are the reasons why Google's MapReduce~\cite{dean:c2008a} framework and its open source re-implementation Hadoop~\cite{hadoop:2008a} became so popular among scientists as well as engineers.
MapReduce is a programming framework for concurrent computations on vast amounts of data running on a large cluster.
Its ingenious idea was separate the computation into a \emph{map} and a \emph{reduce} phase.
In the map phase, the input data set is split into elements which are all processed independently.
Afterwards, the results of the mapper are grouped together and each produced group is given to a reducer.
The reducer knows about all elements in his group and produces the final result.
The strengths of MapReduce are that it is expressive enough to implement a multitude of different algorithms while facilitating at the same time parallel execution.
In fact, the map phase is embarrassingly parallel and the reduce phase only needs a shuffling step prior to its parallel execution.

But still, MapReduce and other frameworks force the user to express the program in a certain way, which is often not natural or intuitive for a user coming from a different domain.
This implies that one has to overcome a particular entry-barrier to use the system and this might already be too high for some users.
Furthermore, the actual program might become lengthy and complicated expressed within the programming model.
This makes developing and debugging the program difficult as well as time-consuming and eventually expensive.
Especially, in the field of data analytics and machine learning programs are usually expressed in a mathematical form.
Therefore, systems such as \matlab~\cite{matlab} and R~\cite{r:1993a} are widely used and recognized for their fast prototyping capabilities and their extensive mathematical libraries.
However, these linear algebra systems lack proper support for automatic parallelization on large clusters and thus restricting the user to a single workstation.
Therefore, the amount of processible data is limited to the size of the main memory, which constitutes a serious drawback for real-world applications.
Moreover, data scientists are usually no experts in the field of distributed computing and people of the distributed computing community often do not know much about data mining.
The group of people being experts in both fields is negligible small.
Consequently, it is very difficult, time-consuming and expensive to implement machine learning and analytic algorithms on distributed systems.

This problem would be mitigated by having a distributed sparse linear algebra system supporting a MATLAB- and R-like language.
Assuming that such a system is realizable, then existing MATLAB- and R-code could be directly run in parallel.
Furthermore, new distributed algorithms could be quickly implemented benefiting from the expressiveness of linear algebra.
This would drastically speed up the application of machine learning and data analysis algorithms to web-scale data.
Due to these reasons, a distributed sparse linear algebra environment, called Gilbert, is developed in the context of this thesis.
In the following, we will present the approach, the encountered problems and the evaluation results of our implementation.