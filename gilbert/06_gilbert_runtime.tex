%!TEX root=main.tex
\chapter{Gilbert Runtime}
\label{cha:gilbertexecution}

\chapterquote{A really great talent finds its happiness in execution.}{Johann Wolfgang von Goethe, (1749 - 1832)}

The Gilbert runtime is responsible for executing the compiled Matlab code on a particular platform.
For this purpose, it receives the intermediate representation of the code and translates it into the execution engine's specific format.
Currently, Gilbert supports three different execution engines.
One of them is the \code{ReferenceExeuctor}, which executes the compiled Matlab code locally.
For the distributed execution Gilbert supports the Spark and the Stratosphere system as backends.

The \code{ReferenceExecutor} is an interpreter for the intermediate code.
It takes the dependency tree of a Matlab program and executes it by evaluating the operators bottom-up.
In order to evaluate an operator, the system first evaluates all the operator's inputs and then executes the actual operator logic.
Since the program is executed locally, the complete data of each operator is always accessible and thus no communication logic is required.
All input and output files are directly written to the local hard drive.

In contrast to the \code{ReferenceExecutor}, the \code{StratosphereExecutor} executes the program distributedly.
It takes the dependency tree of a Matlab program and translates it into a PACT plan.
After the plan is generated, it is issued to the Stratosphere system for parallel execution.
This approach implies that the program is not directly executed by the executor.
Instead, the executor represents just another translation step.

The PACT plans are executed in parallel.
Consequently, we need data structures which can be distributed across several nodes and represent the commonly used linear algebra abstractions, such as vectors and matrices.
Moreover, we have to adjust the linear algebra operations so that they keep working in a distributed environment.
Fortunately, Stratosphere offers a rich and expressive API to easily realize distributed computations.
The details of the distributed data structures and operations are explained in \cref{sec:DistributedMatrixRepresentation} and \cref{sec:LinearAlgebraOperations}.

The \code{SparkExecutor} is the second executor for distributed computations.
In contrast to the \code{StratosphereExecutor}, it executes the Matlab programs on top of the Spark system.
Since Spark and Stratosphere offer a similar set of programming primitives, they can both operate on the same data structures.
Furthermore, even most of the linear algebra operations can be implemented similarly.
The only programming difference is the incremental plan roll out feature of Spark.
By emitting Spark transformations and actions, the user can trigger computations in the cluster and retrieve intermediate results on the driver node.
This allows a more interactive way of programming, manifesting in a more natural way loops are defined, for example.
However, these differences are only subtle and do not limit or extend the expressiveness of either system.

In the following section, we will discuss how Gilbert represents its matrices, allowing parallel execution.
Moreover, we will see how the different linear algebra operations are realized within both systems.

\section{Distributed Matrix Representation}
\label{sec:DistributedMatrixRepresentation}

One aspect of writing distributed algorithms is how the relevant data is distributed across several worker nodes.
Since the distribution pattern directly influences the algorithms, one cannot consider one indenpendently of the other.
In Gilbert's use case, the main data structure are matrices.
Thus, we have to conceive partitioning for matrices which allows efficient partitioning as well as efficient algorithms working on the distributed data.
Looking at a matrix, one easily finds a multitude of different partition schemes.

A first idea could be to partition a matrix according to their rows or columns, as it is depicted in \cref{fig:rowPartitioning} and \cref{fig:columnPartitioning}.
This scheme allows to represent a matrix as a set of vectors which are stored in a distributed fashion.
Furthermore, it allows an efficient realization of cellwise operations, such as $+,-,/$ or $.*$.
In order to calculate the result of such an operation, we only have to join the corresponding rows of both operands and execute the operation locally for each pair of rows.

\begin{figure}
	\centering
	\begin{subfigure}{.3\linewidth}
		\centering
		\includegraphics[width=0.8\textwidth]{images/rowPartitioning.png}
		\caption{Row partitioning}
		\label{fig:rowPartitioning}
	\end{subfigure}
	\begin{subfigure}{.3\linewidth}
		\centering
		\includegraphics[width=0.8\textwidth]{images/columnPartitioning.png}
		\caption{Column partitioning}
		\label{fig:columnPartitioning}
	\end{subfigure}
	\begin{subfigure}{.3\linewidth}
		\centering
		\includegraphics[width=0.8\textwidth]{images/quadraticBlockPartitioning.png}
		\caption{Quadratic block partitioning}
		\label{fig:quadraticBlockPartitioning}
	\end{subfigure}
	\caption{Row-wise, column-wise and quadratic block-wise matrix partitioning.}
	\label{fig:Partitionings}
\end{figure}

However, we run into trouble when we try to multiply two equally partitioned matrices $A$ and $B$ as illustrated in \cref{fig:rowPartitioningMM}.
Here, to calculate the resulting row with index $r$, we need the row $r$ of $A$ and the complete matrix $B$.
This implies a complete repartitioning of $B$.
The repartitioning is especially grave, since $B$ has to be broadcasted to every row of $A$.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{images/rowMM.png}
	\caption{Matrix multiplication of $A$ and $B$. The required data to calculate row $r$ are highlighted.}
	\label{fig:rowPartitioningMM}
\end{figure}

In order to quantify the repartitioning costs of such a matrix multiplication, we developed a simple cost model.
First of all, we limit ourselves to modelling the communication costs, since network I/O usually constitutes the bottleneck of distributed systems and is thus the dominating factor.
For the sake of simplicity, we consider the multiplication of two quadratic matrices $A \in \mathbb{R}^{n\times n}$ and $B \in \mathbb{R}^{n\times n}$ giving the matrix $C\in \mathbb{R}^{n \times n}$.
Moreover, we assume that there are $n$ worker nodes available, each of which receiving a single row of $A$ and $B$ respectively.
Thus, $A$ and $B$ are row-wise partitioned.
We further assume that rows with the same index are kept on the same worker node.

Each row $a_r$ requires the complete knowledge of matrix $B$ in order to produce the row $c_r$.
Therefore, every row $b_r$ has to be sent to all other worker nodes.
Thus, the number of rows sent by each worker node is $n-1$.
All of the $n$ worker nodes have to do this.
Consequently, the total number of sent messages is $n(n-1)$ and each message has a size of $n\alpha$ where $\alpha$ is the size of a matrix entry.
Usually, each sending operation causes some constant overhead inflicted by resource allocation.
Before we can send the actual data over the network, we have to allocate memory to transfer the data to the network interface, reserve network resources and establish a network connection with the remote peer, for example.
This overhead is denoted by $\Delta$.
Since all sending operations occur in parallel, the costs caused by constant overhead are $(n-1)\Delta$.
The total amount of data which has to sent over the network is $n^2(n-1)\alpha$.
We assume that the network interconnection guarantees every node a bandwidth $\nu$.
Therefore, the time needed for sending the data is $\frac{n^2(n-1)\alpha}{\nu}$.
This leads to the following repartitioning cost model:

\begin{displaymath}
	cost_{row} = \frac{n^2(n-1)\alpha}{\nu} + (n-1)\Delta
\end{displaymath}

Row and column partitioning are extreme forms of blocking.
A less extreme form would be to split the matrix into equally sized quadratic blocks as shown in \cref{fig:quadraticBlockPartitioning}.
In order to identify the individual blocks, each of them gets a block row and block column index assigned.
Thus, blocking adds some memory overhead in the form of index information.
An example of a $4\times 4$ matrix partitioned into $2\times 2$ blocks can be seen in \cref{fig:quadraticBlockPartitioningDetailed}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.25\linewidth]{images/quadraticBLockPartitioningDetailed.png}
	\caption{Detailed quadratic block partitioning with the added block row and block column indices.}
	\label{fig:quadraticBlockPartitioningDetailed}
\end{figure}

The blocks are distributed across the worker nodes.
The block size directly controls the granularity of the partitioning.
Increasing the block size will reduce the memory overhead of distributed matrices while reducing the degree of parallelism.
Thus, the user has to adjust the block size value depending on the matrix sizes and the number of available worker nodes in order to obtain best performance.

The quadratic block partitioning has similar properties as the row- and column-wise partitioning scheme when it comes to cellwise operations.
We simply have to join corresponding blocks with respect to the pair of block row and column index and execute the operation on matching blocks locally.
But how does this pattern performs for matrix multiplications?

We make the same assumptions as before and additionally presume that $n$ is a square number.
Since our matrices $A$, $B$ and $C$ are equally partitioned into square blocks, indices will henceforth reference the block and not the matrix entry.
In order to calculate the block $c_{ij}$, we have to know the block row $a_{i}$ and the block column $b_{j}$.
The resulting block will be stored on the node $node_{ij}$ which already contains the blocks $a_{ij}$ and $b_{ij}$.
Thus, each node $node_{ij}$ has to receive the missing $2\left(\sqrt{n}-1\right)$ blocks from the block row $a_{i}$ and block column $b_{j}$.
In total, all worker nodes have to send $2n\left(\sqrt{n}-1\right)$ blocks.
Each block has the size $n\alpha$.
The total communication costs comprises the transmission costs and the network overhead:

\begin{displaymath}
	cost_{squareBlock} = \frac{2n^2\left(\sqrt{n}-1\right)\alpha}{\nu} + 2\left(\sqrt{n}-1\right)\Delta
\end{displaymath}

We see that the term $(n-1)$ is replaced by $2\left(\sqrt{n}-1\right)$ in the square block cost model.
For $n>2$, the square block partitioning scheme is thus superior to the row- and column-wise partitioning pattern with respect to the cost model.
The reason for this is that the square blocks promote more localized computations compared to the other partitionings.
Instead of having to know the complete matrix $B$, we only have to know one block row of $A$ and one block column of $B$ to compute the final result.

Due to these advantages, we decided to implement the square block partitioning scheme in Gilbert.
It would also be possible to combine different partitionings and select them dynamically based on the data sizes and input partitionings.
However, for the sake of simplicity, Gilbert only supports the aforementioned partitioning.

\section{Linear Algebra Operations}
\label{sec:LinearAlgebraOperations}

The other aspect of writing distributed algorithms is to think about how the algorithms work with the locally available data and which data has to be communicated in order to compute the final result.
Stratosphere and Spark both offer a highly expressive programming API which follow and extend the well-known MapReduce paradigm~\cite{dean:c2008a}.
Conceptually, a set of distributed data items, called \emph{DataSet} or \emph{RDD} (resilient distributed dataset) in the context of Stratosphere and Spark respectively, forms the basis of both systems.
There are several ways to initially create such a data set, such as reading from a file or to distribute a local collection.
Once a distributed data set is created, it can be modified by applying one of the transforming functions to it, namely \emph{map}, \emph{reduce}, \emph{join}, \emph{cross} or \emph{coGroup}.
These functions are borrowed from the world of functional programming, since they happen to be expressive enough and can serve as a utile abstraction for automatic parallelization.
Each of these transforming functions takes one or more data sets as inputs and produces a new distributed data set.
The distributed data sets are not computed directly but instead they generate a dataflow plan.
Each node of the dataflow plan constitutes a function application and consequently a new data set.
The edges connect input data sets to output data sets and thus represent the data dependencies.

In order to understand how the different linear algebra operations are implemented, we will quickly revise the semantics of the transforming functions.
For a more detailed explanation, the reader is referred to~\cite{zaharia:2012a,alexandrov:2011a,battre:2010a}.
A distributed data set is a multiset of items.
An item can have a key value which is retrieved by the $key$ function.
Each of the transforming functions is called with a user defined function (UDF), which specifies how each item is processed and what kind of item is emitted as the result.
In other words, it defines the program specific semantics of the transformation.

\begin{description}
	\item[map] The map operator is called with one input data set $A$ and a UDF. The UDF is called for each item $a\in A$ indenpendently, producing one result item.

	\item[reduce] The reduce operator partitions the input data set $A$ into groups of items with the same key. 
		All items of each group are handed together to a call of the UDF.
		In other words, the UDF is called for each submultiset $A^\prime$ with $\forall a,b\in A^\prime : key(a) = key(b)$ and $\forall a \in A^\prime, b\in A \setminus A^\prime: key(a) \not = key(b)$.

	\item[join] The join operator joins two data sets $A$ and $B$ with respect to a key value.
		This means that a UDF is called for each pair $(a,b)$ with $a\in A$ and $b\in B$ with $key(a)=key(b)$.

	\item[cross]The cross operator can be understood as the cartesian product.
		Given two data sets $A$ and $B$, cross calls the UDF for each pair $(a,b)$ with $a\in A$ and $b\in B$.
	\item[coGroup] The coGroup operator also takes 2 input data sets $A$ and $B$.
		It groups the elements of $A$ and $B$ according to their keys and joins the grouped submultisets.
		In other words, the UDF is called for each pair of multisets $(A^\prime, B^\prime)$ with $A^\prime \subseteq A \wedge B^\prime \subseteq B \wedge |keyset(A^\prime \cup B^\prime)| = 1$ and $keyset(A^\prime \cup B^\prime) \cap keyset((A \cup B) \setminus (A^\prime \cup B^\prime)) = \emptyset$.
		The function $keyset$ is the set of appearing keys in a set: $keyset(X) \coloneqq \{ key(x) \mid x \in X \}$.
\end{description}

Stratosphere as well as Spark offer a similar set of $\nth{2}$-order functions comprising the abovementioned functions.
Consequently, we can develop the linear algebra operations in a general fashion and do not have to adhere to a specific framework.
In the following, we will outline the implementation behind the different intermediate operators with respect to the transforming functions.
We assume that the matrices are partitioned using the square block scheme.

The \code{ScalarMatrixTransformation} and \code{MatrixScalarTransformation} work on a matrix input and a scalar input.
The scalar input is a distributed data set with one item.
The scalar value is required at every matrix block to perform the \code{smOp} operation.
Thus, we use the cross function to pair the scalar value with every matrix block.
The resulting dataflow plan is shown in \cref{fig:planScalarMatrixTransformation}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.35\linewidth]{images/planScalarMatrixTransformation.png}
	\caption{Dataflow plan of the \code{ScalarMatrixTransformation}.}
	\label{fig:planScalarMatrixTransformation}
\end{figure}

The \code{CellwiseMatrixTransformation} operates locally on the cells of a single matrix.
Thus, no communication is needed and the \code{unaryScalarOp} operation $f$ can be executed embarrassingly parallely.
The transformation is implemented using the map function.
The resulting dataflow plan is shown in \cref{fig:planCellwiseMatrixTransformation}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.35\linewidth]{images/planCellwiseMatrixTransformation.png}
	\caption{Dataflow plan of the \code{CellwiseMatrixTransformation}.}
	\label{fig:planCellwiseMatrixTransformation}
\end{figure}

The \code{CellwiseMatrixMatrixTransformation} operates locally on the cells of two matrices.
It applies an operation of type \code{scalarOp} to each pair of corresponding cell entries.
Therefore, we have to pair all matrix blocks which have the same block row and block column index.
Assuming, $f$ is the function performing the cellwise operation on the given matrix blocks, the resulting dataflow plan can be seen in \cref{fig:planCellwiseMatrixMatrixTransformation}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.45\linewidth]{images/planCellwiseMatrixMatrixTransformation.png}
	\caption{Dataflow plan of the \code{CellwiseMatrixMatrixTransformation}.}
	\label{fig:planCellwiseMatrixMatrixTransformation}
\end{figure}

Matrix multiplications are probably the most performance critical operation in linear algebra programs.
Therefore, we have to pay attention to a thorough implementation within Gilbert.
In a MapReduce-like system there exist two distinct matrix multiplication implementations for square blocking.
The first approach is based on replicating one of the operands and is called replication based matrix multplication (RMM).
The other method is derived from the outer product formulation of matrix multiplications.
It is called cross product based matrix multiplication (CPMM).

RMM

CPMM

\code{MatrixMult}

The \code{VectorwiseMatrixTransformation} operator cannot be generalized independently of the \code{vectorwiseOp} operation.
For example, consider the maximum and the norm operation.
The maximum is calculated by taking the maximum of each row in block.
Afterwards, the maximum per block is grouped with respect to the block row index and the grouped blocks are reduced by taking the maximum over all group elements.
Grouping is based on the key of each item.
The key selected
The corresponding dataflow plan is shown in \cref{fig:planMaximumVectorwiseTransformation}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.3\linewidth]{images/planMaximumVectorwiseTransformation.png}
	\caption{Dataflow plan of the \code{VectorwiseMatrixTransformation} with the \code{max} operation.}
	\label{fig:planMaximumVectorwiseTransformation}
\end{figure}

In contrast to that, the $2$-norm operation requires a more sophisticated implementation.
First, the cellwise square of all matrix entries is calculated with the map function.
Then, the partial sums of every row is computed by summing the columns of each block.
This constitutes another map operation.
The final row sums are calculated by grouping the partial sums with respect to their block row index and summing the items of each group up.
After this reduce operation is done, the final result is computed by taking the cellwise square root.

The \code{AggregateMatrixTransformation} operator computes an aggregate over all elements of the matrix.
Given that the aggregate operation is combinable, meaning that the aggregation can be expressed as a fold operation in terms of functional programming, we can implement it straightforwardly.
The only thing to do is call a reduce operation with the aggregation function $f$ as UDF.
The respective dataflow plan can be found in \cref{fig:planAggregateMatrixTransformation}.
In case that the aggregate is not combinable, it has to be implemented individually.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.3\linewidth]{images/planAggregateMatrixTransformation.png}
	\caption{Dataflow plan of the \code{AggregateMatrixTransformation}.}
	\label{fig:planAggregateMatrixTransformation}
\end{figure}