%!TEX root=main.tex
\chapter{Gilbert Optimizer}
\label{cha:optimizer}

\chapterquote{Premature optimization is the root of all evil.}{Donald E. Knuth, (born 1938)}

The Gilbert optimizer applies algebraic optimizations to a Gilbert program.
The optimizer exploits equivalence transformations which result from the commutative and associative properties of linear algebra operations.
Therefore, it works on the intermediate format of a program, which provides an appropriate high-level abstraction.

By transforming the intermediate representation, it is often possible to obtain an equivalent algebraic expression with better runtime properties.
Two distinct algebraic expressions, which are semantically equivalent, can differ significantly in terms of memory consumption and required floating-point operations.
Consider, for example, the product of $n$ sums as shown in \cref{eq:productSum}.
The result could be computed with $n$ additions and $n-1$ multiplications.

\begin{equation}
	p = (a_1 + b_1)(a_2 + b_2)\ldots(a_n + b_n) \label{eq:productSum}
\end{equation}

By expanding the right-hand side, we would obtain an equivalent expression.
The expression would be a sum of $2^n$ products, where each product contains $n$ factors.
Thus, we would have to compute $2^n-1$ additions and $2^n(n-1)$ multiplications.
Obviously, the latter expression is far more expensive to compute than the original formulation.

Therefore, it is important for Gilbert to choose the best execution plan.
Currently, the optimizer can apply two optimization strategies: Matrix multiplication reordering and transpose pushdown.
Both transformations will be described in the following sections.

\section{Matrix Multiplication Reordering}

Matrix multiplications belong to the most expensive operations in linear algebra programs.
The naive multiplication of $A\times B$ with $A,B \in \mathbb{R}^{n \times n}$ requires $O(n^3)$ operations.
Even though there are algorithms, such as the Strassen algorithm~\cite{strassen:nm1969a}, which have an asymptotic complexity of $O(n^{\log_28})$, it still remains a costly operation.
In addition to the high computational complexity, the space complexity must also not be forgotten.

In the worst case, the result of a matrix multiplication has quadratic size of the input operands.
This occurs, for example, when we compute the outer product of two vectors.
Given $A \in \mathbb{R}^{n}$, the result $C = A\times A^T \in \mathbb{R}^{n\times n}$ is a complete $n\times n$ matrix.
In practice, this characteristic can render a matrix multiplication infeasible, because there is not enough memory space available.
Or, in case of virtual memory, it will slow down the computation extensively, due to hard disk accesses.

In general, there is not much we can do, if the final result of a matrix multiplication does not fit into memory.
However, if an intermediate result of successive matrix multiplications becomes too huge, there are ways to alleviate the problem.
Usually, by changing the multiplication order, huge intermediate results can be avoided and, thus, also memory shortages.

In order to illustrate the problem, consider the following example:
Assume we want to multiply $A\times B^T \times C$ with $A,B,C\in \mathbb{R}^{n}$.
If we calculated it naively, we would start with $D=A\times B^T \in \mathbb{R}^{n\times n}$.
The result would be $D\times C \in \mathbb{R}^{n}$.
Thus, as an intermediate result, we had to store a $n\times n$ matrix.
Using parentheses to calculate $B^T \times C$ first, we could circumvent the problem.
\begin{displaymath}
	\underbrace{A\times \left( \underbrace{B^T \times C}_{\in \mathbb{R}}\right)}_{\in \mathbb{R}^{n}}
\end{displaymath}
In this case, the biggest matrix to store would be a vector of length $n$.

The best execution order of successive multiplication is that which minimizes the maximum size of intermediate results.
In order to determine the best execution order, the optimizer first extracts all matrix multiplications with more than $2$ operands.
Then, it will calculate for each evaluation order the maximum size of all occurring intermediate results.
In order to do this, the optimizer relies on the automatically inferred matrix sizes of the operands, as described in \cref{sec:MatrixDimensionInference}.
At last, it will pick the execution order with the minimal maximum intermediate matrix size.

\section{Transpose Pushdown}

The idea of transpose pushdown is to move the transpose operations as close to the matrix input as possible.
Thereby, consecutive transpose operations accumulate at the inputs and unnecessary operations erase themselves.
An example is given in \cref{lst:pushdown}.

\begin{listing}[!h]
	\begin{CenteredBox}
		\begin{lstlisting}[language=Matlab]
C = A'*B;
E = (C * D')';
		\end{lstlisting}
	\end{CenteredBox}
	\caption{Transpose pushdown can eliminate unnecessary transpose operations occurring in linear algebra programs.}
	\label{lst:pushdown}
\end{listing}

By inserting $C$ into $E$ we would obtain $E=(A^T BD^T)^T$ which is equivalent to $DB^T A$.
The latter formulation contains only one transpose operation.
Depending on the cost of such an operation the optimization might be worthwhile.

Usually multiple transpose operations occur because they are written for convenience reasons at various positions in the code.
Moreover, in complex programs it is possible that the programmer loses track of them or simply is unaware of the optimization potential.
Therefore, transpose pushdown might be a beneficial optimization.
The performance improvement still needs to be verified and properly quantified.
This will be done in \cref{cha:evaluation}.