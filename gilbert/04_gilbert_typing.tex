%!TEX root=main.tex
\chapter{Gilbert Typing}
\label{cha:gilberttyping}
\chapterquote{Experience without theory is blind, but theory without experience\\ is mere intellectual play.}{Immanuel Kant, (1724 - 1804)}

Program development in general is an error-prone and time-consuming task.
It usually involves besides the actual development phase several iterations of bug fixing.
In order to reduce the number of pitfalls a programmer can fall into, typing systems were developed.
Typing systems assign a \emph{type} to language constructs such as variables, functions and expressions.
That way the system gives meaning to an otherwise vacuous program.
In the memory of a computer, everything is represented as a sequence of bits, no matter whether it is an instruction code, memory address, character, boolean or floating-point number.
For the computer there is no way to intrinsically differentiate between the different meanings without an additional hint.
This hint comes in the form of types.
Knowing that a bit sequence represents a floating-point number, the computer is aware of the valid values and operations and can check for correct usage.

The pursued goal in typing theory is to develop a system which detects erroneous program behaviour and is at the same time \emph{sound} and \emph{complete}.
Soundness means that if a program passes the typer, then it behaves correctly.
Completeness means that if a program behaves correctly, then it will pass the typer.
However, it turned out that typing systems need to be extremely sophisticated in order to detect non-trivial errors and as a result they are often undecidable for non toy example languages.
For example, consider the division operation.
The code \code{1/0} would be well-typed by almost all common type checkers, because integers are divisable.
However, the code will cause a runtime error because of division by zero.
In order to detect this type of error, the type checking would have to be far more detailed.
Therefore, one usually relaxes the constraints.
In general, current type systems can already detect many different errors but they are still incomplete and only partially sound.

Type checking can be distinguished into two categories: \emph{static} and \emph{dynamic} type checking.
Static type checkers work on the source code and assign a type to each expression at compile-time.
If it detects any type incompatibilities, such as providing the wrong arguments to a function call, assigning conflicting data types or to apply not supported operations, the type checker will alert the programmer.
Most type checkers are designed to act conservative, meaning that they relax the constraints of soundness and completeness for the sake of decidability.
It is easy to see that we can reduce the typing problem to a halting problem, if we assume soundness and completeness.
For this purpose consider \cref{lst:typingHaltingProblem}.
In order to decide whether this code is well- or ill-typed, the typer has to decide whether \code{code\_which\_can\_run\_forever()} halts.
Since the halting problem is undecidable, also the set of programs producing a runtime type error is undecidable.
\begin{listing}[!h]
	\begin{CenteredBox}
		\begin{lstlisting}[language=C++]
if(code_which_can_run_forever()){
	code_with_type_error;
}else{
	code_without_type_error;
}
		\end{lstlisting}
	\end{CenteredBox}
	\caption{In order to type this code fragment, the typer has solve the halting problem.}
	\label{lst:typingHaltingProblem}
\end{listing}

In contrast to static typing, dynamic type checkers enrich each object with some kind of type tag which is used to check type compatibility at runtime.
However, possible errors are only recognised after the corresponding code has been executed.
A more thorough discussion about the advantages and disadvantages of both paradigms follows in \cref{sec:staticVSDynamic}.
In practice, there is hardly any static typing system which does not rely at least partially on dynamic typing as well.
Dynamic downcasts, for instance, as they are common in \code{C++}, can only be implemented by checking whether the underlying type is the target type or a subtype of it.

\section{Static Typing vs. Dynamic Typing}
\label{sec:staticVSDynamic}

Static type checking analyzes the program prior to execution to detect errors.
This has the advantage that possible programming mistakes are caught early in the development process.
An assignment of a string to a double would be an example for such a mistake.
As indicated by \cite{westland:jss2002a}, who investigated the influence of errors during software development and found out that unfixed errors become exponentially more costly with each phase, it is important to detect and correct errors as soon as possible.
Static typing usually requires the user to specify types explicitely in the source code, because the language lacks type inference or has ambiguities which prevent the type inference from inferring the correct type.
Java, for example, does not have type inference and therefore the user had specify types redundantly.
In line $1$ of \cref{lst:javaTypeAnnotation}, we see that we have to specify twice the type \code{Object}, even though this can easily be deduced from the right side of the assignment.
Furthermore, in line $4$ we see an addition of two integers.
It is clear that the result is again an integer and thus the \code{int} type specifier is dispensable.
\begin{listing}[!h]
	\begin{CenteredBox}
		\begin{lstlisting}[language=Java]
Object obj = new Object();
int a = 1;
int b = 0;
int c = a + b;			
		\end{lstlisting}
	\end{CenteredBox}
	\caption{Type annotations in Java.}
	\label{lst:javaTypeAnnotation}
\end{listing}

Proponents of static typing emphasize that explicit type information, as they occur in Java and many other programming languages, documents the code.
It is easier for a programmer to use existing code if he can identify function arguments and their types with one glance, for instance. 
Additionally, it is possible for the compiler to apply sophisticated optimization techniques if it knows the types.
The compiler could, for example, use more efficient machine instructions for floating-point calculation if it operates on these values.
Furthermore, it is possible to substitute virtual function calls for direct calls if the actual object type is known.
Another benefit of static typing is the increased type safety.
After passing the type check, the program is guaranteed to fulfill for all inputs some set of type safety properties.
This frees the runtime from checking them and thus the program can be executed more efficiently.
Type information additionally helps to provide a better programming experience in an IDE by offering type dependent context help.
If the IDE knows the type of a variable, then it can tell the prorgammer which methods this type supports, for instance.
And last but not least, explicit typing permits a better abstraction of functionality and thus modularity.
Interfaces can be defined to orchestrate the interaction between several software components allowing to develop them independently from each other.

In contrast to these arguments, advocates of dynamic typing argue that their approach is more vivid and better suited for prototyping, because the static typing approach is too rigid.
This comes especially into effect for programs in a highly dynamic environment with unknown or quickly changing requirements such as data integration.
Another advantage is that the compile time is reduced because of the fewer passes the compiler has to go through.
However, the avoided type checks have to be then realized within the runtime which adds overhead.
But the dynamic nature allows interpreters to dynamically load code more quickly, because all type checkings are deferred until its actual execution.
Moreover, dynamic languages support duck typing as a powerful tool to write reusable code.
The term duck typing originates from the poet James Whitcomb Riley, who stated: \enquote{When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.}~\cite{heim:2007a}.
Essentially, duck typing means that not the actual type of an object decides whether a program is well- or ill-typed but the set of supported methods and its properties.
For example, consider a function which calls a method \code{count} on its single parameter.
Then all function calls are valid which are called with an argument having a method \code{count} independent of its actual type.
That way, the programmer can write code which is applicable for a wide variety of types without having to specify them explicitely.
\textcite{ousterhout:c1998a} even goes further and claims that static typed languages does not guarantee a higher type safety than dynamic typed languages.
Furthermore, they are more verbose and it is difficult to write reusable code.
However, it is unclear in which way reducing language features leads to a more powerful and expressive language. 

The debate of whether statically or dynamically typed languages are superior has almost reached religious character.
It is likely that none of the approaches alone solves all problems.
Instead, a combination of the strengths of both paradigms promises the best results for the aforementioned problems~\cite{meijer:2004a}.

We are in the unfortunate situation that Matlab belongs to the class of dynamically typed languages and Gilbert requires type information for the parallel execution.
The reason why Gilbert needs type information is that the parallel data processing systems, used to run Gilbert distributedly, have to know which data types are passed from one worker node to another.
Therefore, we did not have any choice and had to enrich Matlab with type information.

There exists research of how to add explicit type information to Matlab.
For example, \textcite{hendren:2011a} introduced the special keyword \emph{atype} to Matlab which are understood and interpreted by an extended compiler.
The atype keyword basically acts as a type annotation and can be implemented within a special library or as a weaver.
Even though this approach seems quite promising, we decided to opt for a more transparent mechanism, namely type inference.
Type inference has the advantage that the Matlab user is not bothered by adding explicit type information and thus can continue writing his code in the usual fashion.
In the case that the type inference algorithm cannot properly infer the types, there has to be a way to resolve this problem.
We decided to pursue a similar approach as \textcite{furr:2009a}.
\citeauthor{furr:2009a} added type information to Ruby by adding special comments to the respective code sections.
Thereby, the code does not break with the Ruby standard and still contains type information.
As typing system, we use the Hindley-Milner (HM) type system~\cite{hindley:tams1969a,milner:jcss1978a} and a slightly derived form of algorithm W~\cite{damas:1982a} for type inference which will be described in the next section in detail.
Even though there exist more powerful type systems than the HM type system, it has the appealing charm that the algorithm W is sound, complete and decidable with respect to the type system.
Furhermore, it has proven to type several algorithms implemented within Gilbert correctly.

\section{Hindley-Milner Type Inference}

The Hindley-Milner (HM) type inference assigns types to expressions.
It was initially developed to type functional languages.
In fact, HM type inference was first implemented as the typing part of the programming language ML.
HM types the expressions of the lambda calculus enriched with the \code{let}-expression.
For a detailed review of the lambda calculus, the interested reader is referred to the article of \textcite{cardone:hhl2006a}.

The set of expressions $e$ contain a variable, a function application, a function abstraction and the \code{let}-expression.
The formal definition of $e$ is
\begin{eqnarray*}
e &= & x\quad\text{(Variable)}\\
&|& e\ e\quad\text{(Application)}\\
&|& \lambda x. e\quad\text{(Abstraction)}\\
&|& \code{let } x = e \code{ in } e
\end{eqnarray*}
For those unconversant with lambda expressions, we will quickly revise them.
The function application is written as $e_1\ e_2$ whereas $e_1$ denotes a function and $e_2$ the function argument which is applied to the function body.
The function abstraction $\lambda x.e$ is the equivalent of an anonymous function as it is known from many program languages.
It is initiated with a $\lambda$ followed by its function parameter $x$ and the function body $e$.
The let-expression $\code{let } x = e_1 \code{ in } e_2$ introduces a new variable $x$, having the value $e_1$, into the context $\Gamma$.
This variable can be used within the expression $e_2$.
The semantic of the assignment is that every occurence of $x$ in $e_2$ is replaced by $e_1$.
The context $\Gamma$ contains all type information so far known and is a mapping from variables to types.
A more precise definition is given in a later paragraph.

One might think that the lambda calculus is only a toy language and that the set of expressions is not expressive enough to represent a common programming problem.
However, it turned out that the lambda calculus is Turing complete and thus any programming language can be reduced to it.
Therefore, it is enough to reason about the typing aspects of these expressions.

Even though Gilbert is not a functional language, we can use the HM type inference to compute the types at compile-time.
The only difference to the above defined set of expressions is that Gilbert does not have a \code{let}-expression.
Instead, it has ordinary assignments of the form $x = e$.
The notion of the assignment is similar to the \code{let}-expression.
An assignment adds a new variable $x$ with the value $e$ to the context.

HM type inference distinguishes between two sorts of types, \emph{mono}- and \emph{polytypes}.
Monotypes $\tau$ are defined as following:
\begin{eqnarray*}
\tau &=& \alpha\quad\text{(Variable)}\\
&|& D\ \tau\ \ldots \tau\quad\text{(Application)}
\end{eqnarray*}
A monotype always denotes a concrete type.
If the type is known, it can be a type constant, such as \code{int}, \code{float} or \code{string}, or a parametric type.
A parametric type takes itself type arguments to form a concrete type.
An example of a parametric type is the \code{Set} or the \code{List} which can be instantiated with arbitrary element types.
Type constants are a special case of the application rule with an empty list of arguments.
If the type not yet known but it can only be a single type, then use the type variable $\alpha$.
Often it can be the case that these type variables are replaced with known types at a later stage of the type inference.

In contrast to monotypes, polytypes $\sigma$ denote multiple types.
They are defined by
\begin{eqnarray*}
\sigma &=& \tau \\
&|& \forall \alpha .\sigma
\end{eqnarray*}
.
A type of the form $\forall \alpha.\sigma$ indicates the set of all types where $\alpha$ is substituted by a concrete type $\tau$ within $\sigma$.
Consider for example the tuple access function \code{fst} which takes a tuple and returns the first entry.
The function \code{fst} should be applicable to all different types of tuples, for example $(\code{int}, \code{int})$, $(\code{string}, \code{float})$, etc.
Thus, \code{fst} cannot have a monotype, because then it would only work for one concrete tuple type.
The actual type of \code{fst} is $\forall \alpha,\beta . (\alpha, \beta) \rightarrow \alpha$.
Depending on the applied tuple argument the quantified type variables are instantiated respectively.
Polytypes permit to implement generic functions in a type safe manner and this is called parametric polymorphism.

Usually, expressions depend on other expression and thus their types, too.
Therefore, we a kind of type dictionary where we note types of expressions we have already seen while parsing the code.
And this is exactly what the context $\Gamma$ is used for.
$\Gamma$ contains a list of pairs $x:\sigma$, which are called assumptions.
The formal definition of $\Gamma$ is
\begin{eqnarray*}
\Gamma &=& \Gamma, x:\sigma \\
&|& \epsilon\ \text{(empty)}
\end{eqnarray*}
.
Having defined the context, expressions and types we can now specify the typing judgment $\Gamma \vdash x : \sigma$.
It means that given the context $\Gamma$ the variable $x$ has the type $\sigma$.
The way how to come up with these judgements is defined by deduction rules.
Before we will take a closer look at them, we first have to introduce some auxiliary functions.

An important distinction in a HM type expression is the state of the type variables.
In general, type variables can appear as \emph{free} or as \emph{bound} variables.
A type variable $\alpha$ is called bound if it occurs in an expression of the form $\forall \alpha. \tau$.
This means that $\forall$ binds the subsequent variable in the context of the expression.
All variables which are not bound are free.
We can define a function $free$ which calculates the set of free type variables in a type expression.
\begin{eqnarray*}
free(\alpha) &=& \{\alpha\}\\
free(D\ \alpha_1\ \ldots\ \alpha_n) &=& \bigcup_{i=1}^n free(\alpha_i)\\
free(\forall \alpha. \tau) &=& free(\tau) \setminus \{\alpha\}\\
free(\Gamma) &=& \bigcup_{x:\sigma \in \Gamma} free(\sigma)
\end{eqnarray*}

\subsection{Algorithm W}

\begin{prooftree}
	\AxiomC{$x : \sigma \in \Gamma$}
	\AxiomC{$\tau = inst(\sigma)$}
	\LeftLabel{Variable:\quad}
	\BinaryInfC{$\Gamma \vdash x : \tau$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\Gamma \vdash e_0 : \tau_0$}
	\AxiomC{$\Gamma \vdash e_1 : \tau_1$}
	\AxiomC{$\tau^\prime=newvar$}
	\AxiomC{$unify(\tau_0, \tau_1 \rightarrow \tau^\prime)$}
	\LeftLabel{Application:\quad}
	\QuaternaryInfC{$\Gamma \vdash e_0 e_1 : \tau^\prime$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\tau = newvar$}
	\AxiomC{$\Gamma,x:\tau \vdash e : \tau^\prime$}
	\LeftLabel{Abstraction:\quad}
	\BinaryInfC{$\Gamma \vdash \lambda x.e:\tau \rightarrow \tau^\prime$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\Gamma \vdash e_0 : \tau$}
	\AxiomC{$\Gamma, x : \overline{\Gamma}(\tau) \vdash e_1 : \tau^\prime$}
	\LeftLabel{Assignment:\quad}
	\BinaryInfC{$\Gamma \vdash x = e_0; e_1 : \tau^\prime$}
\end{prooftree}

\section{Matrix Dimension Inference}