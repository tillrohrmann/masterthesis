%!TEX root=main.tex
\chapter{Gilbert Typing}
\label{cha:gilberttyping}
\chapterquote{Experience without theory is blind, but theory without experience\\ is mere intellectual play.}{Immanuel Kant, (1724 - 1804)}

Program development in general is an error-prone and time-consuming task.
It usually involves besides the actual development phase several iterations of bug fixing.
In order to reduce the number of pitfalls a programmer can fall into, typing systems were developed.
Typing systems assign a \emph{type} to language constructs such as variables, functions and expressions.
That way the system gives meaning to an otherwise vacuous program.
In the memory of a computer, everything is represented as a sequence of bits, no matter whether it is an instruction code, memory address, character, boolean or floating-point number.
For the computer there is no way to intrinsically differentiate between the different meanings without an additional hint.
This hint comes in the form of types.
Knowing that a bit sequence represents a floating-point number, the computer is aware of the valid values and operations and can check for correct usage.

The pursued goal in typing theory is to develop a system which detects erroneous program behavior and is at the same time \emph{sound} and \emph{complete}.
Soundness means that if a program passes the typer, then it behaves correctly.
Completeness means that if a program behaves correctly, then it will pass the typer.
However, it turned out that typing systems need to be extremely sophisticated in order to detect non-trivial errors and as a result they are often undecidable for non toy example languages.
For example, consider the division operation.
The code \code{1/0} would be well-typed by almost all common type checkers, because integers are divisible.
However, the code will cause a runtime error because of division by zero.
In order to detect this type of error, the type checking would have to be far more detailed.
Therefore, one usually relaxes the constraints.
In general, current type systems can already detect many different errors but they are still incomplete and only partially sound.

Type checking can be distinguished into two categories: \emph{static} and \emph{dynamic} type checking.
Static type checkers work on the source code and assign a type to each expression at compile-time.
If it detects any type incompatibilities, such as providing the wrong arguments to a function call, assigning conflicting data types or to apply not supported operations, the type checker will alert the programmer.
Most type checkers are designed to act conservative, meaning that they relax the constraints of soundness and completeness for the sake of decidability.
It is easy to see that we can reduce the typing problem to a halting problem, if we assume soundness and completeness.
For this purpose consider \cref{lst:typingHaltingProblem}.
In order to decide whether this code is well- or ill-typed, the typer has to decide whether \code{code\_which\_can\_run\_forever()} halts.
Since the halting problem is undecidable, also the set of programs producing a runtime type error is undecidable.
\begin{listing}[!h]
	\begin{CenteredBox}
		\begin{lstlisting}[language=C++]
if(code_which_can_run_forever()){
	code_with_type_error;
}else{
	code_without_type_error;
}
		\end{lstlisting}
	\end{CenteredBox}
	\caption{In order to type this code fragment, the typer has solve the halting problem.}
	\label{lst:typingHaltingProblem}
\end{listing}

In contrast to static typing, dynamic type checkers enrich each object with some kind of type tag which is used to check type compatibility at runtime.
However, possible errors are only recognized after the corresponding code has been executed.
A more thorough discussion about the advantages and disadvantages of both paradigms follows in \cref{sec:staticVSDynamic}.
In practice, there is hardly any static typing system which does not rely at least partially on dynamic typing as well.
Dynamic downcasts, for instance, as they are common in \code{C++}, can only be implemented by checking whether the underlying type is the target type or a subtype of it.

\section{Static Typing vs. Dynamic Typing}
\label{sec:staticVSDynamic}

Static type checking analyzes the program prior to execution to detect errors.
This has the advantage that possible programming mistakes are caught early in the development process.
An assignment of a string to a double would be an example for such a mistake.
As indicated by \cite{westland:jss2002a}, who investigated the influence of errors during software development and found out that unfixed errors become exponentially more costly with each phase, it is important to detect and correct errors as soon as possible.
Static typing usually requires the user to specify types explicitly in the source code, because the language lacks type inference or has ambiguities which prevent the type inference from inferring the correct type.
Java, for example, does not have type inference and therefore the user had specify types redundantly.
In line $1$ of \cref{lst:javaTypeAnnotation}, we see that we have to specify twice the type \code{Object}, even though this can easily be deduced from the right side of the assignment.
Furthermore, in line $4$ we see an addition of two integers.
It is clear that the result is again an integer and thus the \code{int} type specifier is dispensable.
\begin{listing}[!h]
	\begin{CenteredBox}
		\begin{lstlisting}[language=Java]
Object obj = new Object();
int a = 1;
int b = 0;
int c = a + b;			
		\end{lstlisting}
	\end{CenteredBox}
	\caption{Type annotations in Java.}
	\label{lst:javaTypeAnnotation}
\end{listing}

Proponents of static typing emphasize that explicit type information, as they occur in Java and many other programming languages, documents the code.
It is easier for a programmer to use existing code if he can identify function arguments and their types with one glance, for instance. 
Additionally, it is possible for the compiler to apply sophisticated optimization techniques if it knows the types.
The compiler could, for example, use more efficient machine instructions for floating-point calculation if it operates on these values.
Furthermore, it is possible to substitute virtual function calls for direct calls if the actual object type is known.
Another benefit of static typing is the increased type safety.
After passing the type check, the program is guaranteed to fulfill for all inputs some set of type safety properties.
This frees the runtime from checking them and thus the program can be executed more efficiently.
Type information additionally helps to provide a better programming experience in an IDE by offering type dependent context help.
If the IDE knows the type of a variable, then it can tell the programmer which methods this type supports, for instance.
And last but not least, explicit typing permits a better abstraction of functionality and thus modularity.
Interfaces can be defined to orchestrate the interaction between several software components allowing to develop them independently from each other.

In contrast to these arguments, advocates of dynamic typing argue that their approach is more vivid and better suited for prototyping, because the static typing approach is too rigid.
This comes especially into effect for programs in a highly dynamic environment with unknown or quickly changing requirements such as data integration.
Another advantage is that the compile time is reduced because of the fewer passes the compiler has to go through.
However, the avoided type checks have to be then realized within the runtime which adds overhead.
But the dynamic nature allows interpreters to dynamically load code more quickly, because all type checkings are deferred until its actual execution.
Moreover, dynamic languages support duck typing as a powerful tool to write reusable code.
The term duck typing originates from the poet James Whitcomb Riley, who stated: \enquote{When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.}~\cite{heim:2007a}.
Essentially, duck typing means that not the actual type of an object decides whether a program is well- or ill-typed but the set of supported methods and its properties.
For example, consider a function which calls a method \code{count} on its single parameter.
Then all function calls are valid which are called with an argument having a method \code{count} independent of its actual type.
That way, the programmer can write code which is applicable for a wide variety of types without having to specify them explicitly.
\textcite{ousterhout:c1998a} even goes further and claims that static typed languages does not guarantee a higher type safety than dynamic typed languages.
Furthermore, they are more verbose and it is difficult to write reusable code.
However, it is unclear in which way reducing language features leads to a more powerful and expressive language. 

The debate of whether statically or dynamically typed languages are superior has almost reached religious character.
It is likely that none of the approaches alone solves all problems.
Instead, a combination of the strengths of both paradigms promises the best results for the aforementioned problems~\cite{meijer:2004a}.

We are in the unfortunate situation that Matlab belongs to the class of dynamically typed languages and Gilbert requires type information for the parallel execution.
The reason why Gilbert needs type information is that the parallel data processing systems, used to run Gilbert in parallel, have to know which data types are passed from one worker node to another.
Therefore, we did not have any choice and had to enrich Matlab with type information.

There exists research of how to add explicit type information to Matlab.
For example, \textcite{hendren:2011a} introduced the special keyword \emph{atype} to Matlab which are understood and interpreted by an extended compiler.
The atype keyword basically acts as a type annotation and can be implemented within a special library or as a weaver.
Even though this approach seems quite promising, we decided to opt for a more transparent mechanism, namely type inference.
Type inference has the advantage that the Matlab user is not bothered by adding explicit type information and thus can continue writing his code in the usual fashion.
In the case that the type inference algorithm cannot properly infer the types, there has to be a way to resolve this problem.
We decided to pursue a similar approach as \textcite{furr:2009a}.
\citeauthor{furr:2009a} added type information to Ruby by adding special comments to the respective code sections.
Thereby, the code does not break with the Ruby standard and still contains type information.
As typing system, we use the Hindley-Milner (HM) type system~\cite{hindley:tams1969a,milner:jcss1978a} and a slightly derived form of algorithm W~\cite{damas:1982a} for type inference which will be described in the next section in detail.
Even though there exist more powerful type systems than the HM type system, it has the appealing charm that the algorithm W is sound, complete and decidable with respect to the type system.
Furthermore, it has proven to type several algorithms implemented within Gilbert correctly.

\section{Hindley-Milner Type Inference}
\label{sec:hmInference}

The Hindley-Milner (HM) type inference assigns types to expressions.
It was initially developed to type functional languages.
In fact, HM type inference was first implemented as the typing part of the programming language ML.
HM types the expressions of the lambda calculus enriched with the \code{let}-expression.
For a detailed review of the lambda calculus, the interested reader is referred to the article of \textcite{cardone:hhl2006a}.

The set of expressions $e$ contain a variable, a function application, a function abstraction and the \code{let}-expression.
The formal definition of $e$ is
\begin{eqnarray*}
e &= & x\quad\text{(Variable)}\\
&|& e\ e\quad\text{(Application)}\\
&|& \lambda x. e\quad\text{(Abstraction)}\\
&|& \code{let } x = e \code{ in } e
\end{eqnarray*}
For those unconversant with lambda expressions, we will quickly revise them.
The function application is written as $e_1\ e_2$ whereas $e_1$ denotes a function and $e_2$ the function argument which is applied to the function body.
The function abstraction $\lambda x.e$ is the equivalent of an anonymous function as it is known from many program languages.
It is initiated with a $\lambda$ followed by its function parameter $x$ and the function body $e$.
The let-expression $\code{let } x = e_1 \code{ in } e_2$ introduces a new variable $x$, having the value $e_1$, into the context $\Gamma$.
This variable can be used within the expression $e_2$.
The semantic of the assignment is that every occurrence of $x$ in $e_2$ is replaced by $e_1$.
The context $\Gamma$ contains all type information so far known and is a mapping from variables to types.
A more precise definition is given in a later paragraph.

One might think that the lambda calculus is only a toy language and that the set of expressions is not expressive enough to represent a common programming problem.
However, it turned out that the lambda calculus is Turing complete and thus any programming language can be reduced to it.
Therefore, it is enough to reason about the typing aspects of these expressions.

Even though Gilbert is not a functional language, we can use the HM type inference to compute the types at compile-time.
The only difference to the above defined set of expressions is that Gilbert does not have a \code{let}-expression.
Instead, it has ordinary assignments of the form $x = e$.
The notion of the assignment is similar to the \code{let}-expression.
An assignment adds a new variable $x$ with the value $e$ to the context.

HM type inference distinguishes between two sorts of types, \emph{mono}- and \emph{polytypes}.
Monotypes $\tau$ are defined as following:
\begin{eqnarray*}
\tau &=& \alpha\quad\text{(Variable)}\\
&|& D\ \tau\ \ldots \tau\quad\text{(Application)}
\end{eqnarray*}
A monotype always denotes a concrete type.
If the type is known, it can be a type constant, such as \code{int}, \code{float} or \code{string}, or a parametric type.
A parametric type takes itself type arguments to form a concrete type.
An example of a parametric type is the \code{Set} or the \code{List} which can be instantiated with arbitrary element types.
Type constants are a special case of the application rule with an empty list of arguments.
If the type not yet known but it can only be a single type, then use the type variable $\alpha$.
Often it can be the case that these type variables are replaced with known types at a later stage of the type inference.

In contrast to monotypes, polytypes $\sigma$ denote multiple types.
They are defined by
\begin{eqnarray*}
\sigma &=& \tau \\
&|& \forall \alpha .\sigma
\end{eqnarray*}
.
A type of the form $\forall \alpha.\sigma$ indicates the set of all types where $\alpha$ is substituted by a concrete type $\tau$ within $\sigma$.
Consider for example the tuple access function \code{fst} which takes a tuple and returns the first entry.
The function \code{fst} should be applicable to all different types of tuples, for example $(\code{int}, \code{int})$, $(\code{string}, \code{float})$, etc.
Thus, \code{fst} cannot have a monotype, because then it would only work for one concrete tuple type.
The actual type of \code{fst} is $\forall \alpha,\beta . (\alpha, \beta) \rightarrow \alpha$.
Depending on the applied tuple argument the quantified type variables are instantiated respectively.
Polytypes permit to implement generic functions in a type safe manner and this is called parametric polymorphism.
It is important to note that polytypes introduce ambiguities with respect to the calculated type.
An expression having the type $\forall \alpha. \alpha$ also has the type $\code{int}$, for example.
Thus, the latter type would be a valid inferred type, even though the first one is more general.
In order to dissolve this ambiguity, the typing system always looks for the most general type.

Usually, expressions depend on other expression and thus their types, too.
Therefore, we a kind of type dictionary where we note types of expressions we have already seen while parsing the code.
And this is exactly what the context $\Gamma$ is used for.
$\Gamma$ contains a list of pairs $x:\sigma$, which are called assumptions.
The formal definition of $\Gamma$ is
\begin{eqnarray*}
\Gamma &=& \Gamma, x:\sigma \\
&|& \epsilon\ \text{(empty)}
\end{eqnarray*}
.
Having defined the context, expressions and types we can now specify the typing judgment $\Gamma \vdash x : \sigma$.
It means that given the context $\Gamma$ the variable $x$ has the type $\sigma$.
The way how to come up with these judgments is defined by deduction rules.
Before we will take a closer look at them, we first have to introduce some auxiliary functions.

An important distinction in a HM type expression is the state of the type variables.
In general, type variables can appear as \emph{free} or as \emph{bound} variables.
A type variable $\alpha$ is called bound if it occurs in an expression of the form $\forall \alpha. \tau$.
This means that $\forall$ binds the subsequent variable in the context of the expression.
All variables which are not bound are free.
We can define a function $free$ which calculates the set of free type variables in a type expression and in the context.
\begin{eqnarray*}
free(\alpha) &=& \{\alpha\}\\
free(D\ \alpha_1\ \ldots\ \alpha_n) &=& \bigcup_{i=1}^n free(\alpha_i)\\
free(\forall \alpha. \tau) &=& free(\tau) \setminus \{\alpha\}\\
free(\Gamma) &=& \bigcup_{x:\sigma \in \Gamma} free(\sigma)
\end{eqnarray*}

\subsection{Algorithm W}

The algorithm W was initially developed by \textcite{damas:1982a} and allows to solve the HM type inference in almost linear time with respect to the size of the source code.
Thus, it is also applicable to large programs.
We present a modified version of the original algorithm which incorporates side effects into the deduction rules.
Even though this contradicts the purity of logical deduction rules, it allows us to express the algorithm concisely.
The algorithm W is defined by the following deduction rules:

\begin{prooftree}
	\AxiomC{$x : \sigma \in \Gamma$}
	\AxiomC{$\tau = inst(\sigma)$}
	\LeftLabel{Variable:\quad}
	\BinaryInfC{$\Gamma \vdash x : \tau$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\Gamma \vdash e_0 : \tau_0$}
	\AxiomC{$\Gamma \vdash e_1 : \tau_1$}
	\AxiomC{$\tau^\prime=newvar$}
	\AxiomC{$unify(\tau_0, \tau_1 \rightarrow \tau^\prime)$}
	\LeftLabel{Application:\quad}
	\QuaternaryInfC{$\Gamma \vdash e_0 e_1 : \tau^\prime$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\tau = newvar$}
	\AxiomC{$\Gamma,x:\tau \vdash e : \tau^\prime$}
	\LeftLabel{Abstraction:\quad}
	\BinaryInfC{$\Gamma \vdash \lambda x.e:\tau \rightarrow \tau^\prime$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\Gamma \vdash e_0 : \tau$}
	\AxiomC{$\Gamma, x : \overline{\Gamma}(\tau) \vdash e_1 : \tau^\prime$}
	\LeftLabel{Assignment:\quad}
	\BinaryInfC{$\Gamma \vdash x = e_0; e_1 : \tau^\prime$}
\end{prooftree}
.
Each rule has the form
\begin{tabular}{c}
Premise\\
\hline
Conclusion
\end{tabular}, which means that we have to prove the premise in order to show the conclusion.

Let us go step by step through the different deduction rules of the algorithm.
The first rule says that a variable $x$ has the type $\tau$, if we can find an assumption of $x$ in $\Gamma$ whose type $\sigma$ can be specialized to $\tau$.
The function $inst$ does the specialization of the type.
A type is specialized by removing all quantifiers and replacing the bound variables by fresh monotype variables.

The second rule deals with function applications.
In order to show that a function application has the type $\tau^\prime$, we first have to look at the function type $\tau_0$ and the argument type $\tau_1$.
In order to fulfill the conclusion, we know that the type of $e_0$ has to be a function with an argument of type $\tau_1$ and some result type $\tau^\prime$.
If the type $\tau_0$ is equivalent to this function type $\tau_1\rightarrow\tau^\prime$, we can deduce that the resulting type is $\tau^\prime$.
The equivalence is tested by the $unify$ function.
This function takes two types and tries to find the most general type unifying both arguments.

The $unify$ function is an essential part of the typing system, since it incorporates all constraints imposed by the different expressions in order to find the most general type validating the program.
The working principle can best understood by looking at its pseudocode in \cref{lst:unify}.
\begin{listing}[!h]
	\begin{CenteredBox}
		\begin{lstlisting}[language=inform]
def unify(typeA: Type, typeB: Type): Type = {
	val tA = resolve(typeA)
	val tB = resolve(typeB)

	if( at least one term is a type variable){
		return union(tA, tB)
	}
    else if( both type expressions are of the form D p1 ... pn with 
		the same D and the number of arguments){
		return D(unify(tA.p1, tB.p1), ..., unify(tA.pn, tb.pn))
	} else{
		return typeMismatch
	}
}
		\end{lstlisting}
	\end{CenteredBox}
	\caption{$Unify$ function.}
	\label{lst:unify}
\end{listing}

First of all both type expressions are resolved.
This means that type variables contained as sub expressions are looked up in a type variable assignment dictionary.
This dictionary maintains the current assignments between variables and their types.
If a type variable has an assigned type, then this type is substituted for the corresponding type variable.
If at least one of the resulting type expressions is still a type variable, then we can simply construct the union of both types.
The union function returns the more concrete type of both arguments and updates the dictionary correspondingly.
Thus, if both arguments are type variables, then one of them is returned.
If only one argument is a type variable, then the other argument is returned.

If none of the resolved type expressions is a type variable, then they have to be a type application.
Applications can only be unified if they are constructed by the same function symbol and if they have the same number of arguments.
If this condition is fulfilled, we can try to unify the arguments.
Depending on whether the arguments are unifiable or not, the overall type application is unifiable or not.

If both type expressions are constructed by different type applications, we know that the types are impossible to unify.
Consequently, we can directly return a type mismatch error.

The fourth rule allows to type lambda abstractions or in our case concrete and anonymous functions.
In order to show that a lambda abstraction $\lambda x.e$ has the type $\tau\rightarrow \tau^\prime$, we first define a new type variable for the unknown parameter type.
By adding this assumption to the context, we then have to calculate the type of the function body $e$.
If we can prove that $e$ has the type $\tau^\prime$, we can finally conclude that the lambda abstraction has the type $\tau \rightarrow \tau^\prime$.

And the last deduction rule covers assignments.
An assignment simply binds a variable name $x$ to an expression value.
Since this expression value owns a type $\tau$, we can add a new assumption $x:\tau$ to the context.
We continue the type inference for the subsequent expressions with this updated type context.
That is expressed by the deduction rule.
In fact, there is slight difference.
Instead of adding the assignment $x:\tau$ we add $x:\overline{\Gamma}(\tau)$ to the context.
$\overline{\Gamma}(\tau)$ calculates the generalization of $\tau$.
This means that all free monotypes of $\tau$ not bound in $\Gamma$ are quantified.
That way, the most general type is found.
\begin{eqnarray*}
	\overline{\Gamma}(\tau) &=& \forall \overline{\alpha}. \tau\\
	\text{with}\quad\overline{\alpha} &=& free(\tau) \setminus free(\Gamma)
\end{eqnarray*}

\section{Matrix Dimension Inference}
\label{sec:MatrixDimensionInference}

Matrices constitute the elementary data type in our linear algebra environment.
Besides its element type, a matrix is also defined by its size.
In the context of program execution, knowledge about matrix sizes can help a lot to optimize the evaluation.
For instance, consider a threefold matrix multiplication $A\times B\times C$.
The multiplication can be evaluated in two different ways: $(A\times B)\times C$ and $A\times(B\times C)$.
For certain matrix sizes one way might be infeasible whereas the other way can be calculated efficiently due to the matrix size of the intermediate result $(A\times B)$ or $(B\times C)$.

To illustrate this point, assume that $A\in \mathbb{R}^{1000\times 10}, B\in \mathbb{R}^{10\times 1000}$ and $C \in \mathbb{R}^{10\times 1000}$.
Thus, the intermediate products are:
\begin{eqnarray*}
	A\times B &\in& \mathbb{R}^{1000 \times 1000}\\
	B\times C &\in& \mathbb{R}^{10 \times 10}
\end{eqnarray*}
.
In the first case we have to calculate a $1000 \times 1000$ matrix and in the second only a $10 \times 10$ matrix.

By knowing the matrix sizes, we can choose the most efficient strategy to calculate the requested result.
Another advantage is that we can decide whether to carry out the computation in-core or in parallel depending on the matrix sizes.
Sometimes the benefit of parallel execution is smaller than the initial communication overhead and thus it would be wiser to execute the calculation locally.
Furthermore, it can be helpful for data partitioning on a large cluster and to decide on a blocking strategy with respect to the algebraic operations.
Therefore, we extended the HM type inference to also infer matrix sizes where possible.

Gilbert's matrix type is defined as 
\begin{displaymath}
MatrixType(\underbrace{\tau}_{\text{Element type}},\underbrace{\nu}_{\text{Number of rows}},\underbrace{\nu}_{\text{Number of columns}})
\end{displaymath}
with $\nu$ being the value type:
\begin{eqnarray*}
	\nu &=& \gamma\quad\text{(Variable)} \\
	&|& \delta\quad\text{(Value)}
\end{eqnarray*}
.
The value type can either be a value variable or a concrete value.

Value variables always appear if we know that a certain relationship holds, but do not know the concrete values yet.
For example, when we multiply two matrices $A\in\mathbb{R}^{a\times b}$ and $B\in\mathbb{R}^{b\times c}$ we know that the result is $A\times B = C \in \mathbb{R}^{a\times c}$, without having knowledge about the actual values.
Once we have deduced the actual values for a variable we can simply replace them.

One of these occasions is the definition of a matrix.
Gilbert implements several of these functions, known from Matlab: \code{eye}, \code{zeros}, \code{ones} or the \code{load} function, reading a matrix from disk.
All of these functions require to specify the number of rows and columns for the resulting matrix.
These values often serve as the source of our deduction.
Another possibility are special operations which modify the size of the matrix in a deterministic way.
The sum operation, which calculates row or column sums respectively, will reduce the dimension which is used for the summation to $1$.

The matrix size inference is incorporated into the HM type inference by adding some logic to the \code{unify} function.
Whenever we encounter a matrix type during the unification process, we call a \code{unifyValue} function on the two row and column values.
The \code{unifyValue} function works similarly to the \code{unify} function.
First, the function resolves the value expression, thus substituting value variables with their assigned values.
Then if at least one of the resolved value expressions is still a value variable, then the union is constructed and the corresponding value variable dictionary entry is updated.
If both resolved expressions are equal, then this value is returned and otherwise a value mismatch error is thrown.